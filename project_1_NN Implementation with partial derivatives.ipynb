{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Georgios Cheirmpos - 3130230\n",
    "#Machine Learning Project 1\n",
    "#PROJECT DIRECTORY MUST BE LIKE BELOW\n",
    "#cifar-10-batches-py/<contents of cifar zip>\n",
    "#mnistdata/<mnistfiles>\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.utils\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_mnist():\n",
    "    df = None  \n",
    "    y_train = []\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'mnistdata/train%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        \n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_train.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "    train_data = df.as_matrix()\n",
    "    y_train = np.array( y_train )    \n",
    "    #load test files\n",
    "    df = None    \n",
    "    y_test = []\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'mnistdata/test%d.txt' % i, header=None, sep=\" \" )        \n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]       \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_test.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "    test_data = df.as_matrix()\n",
    "    y_test = np.array( y_test )    \n",
    "    return train_data, test_data, y_train, y_test\n",
    "\n",
    "def load_data_cifar():\n",
    "    y_train = []\n",
    "    X_train = None\n",
    "    for i in range( 1,6 ):\n",
    "        tmp = pd.read_pickle('cifar-10-batches-py/data_batch_%d' % i)\n",
    "        if i == 1 :\n",
    "            X_train = tmp['data']    \n",
    "        else:\n",
    "            X_train = np.vstack((X_train, tmp['data']))\n",
    "\n",
    "        for k in range(len(tmp['labels'])):\n",
    "            hot_vector = [ 1 if j == tmp['labels'][k] else 0 for j in range(0,10) ]\n",
    "            y_train.append(hot_vector)\n",
    "        #concatenate dataframes by rows    \n",
    "    y_train = np.array( y_train )\n",
    "    #load test files\n",
    "    X_test = None    \n",
    "    y_test = []\n",
    "    tmp = pd.read_pickle('cifar-10-batches-py/test_batch')\n",
    "    #build labels - one hot vector\n",
    "    X_test = tmp['data'] \n",
    "    for k in range(len(tmp['labels'])):\n",
    "        hot_vector = [ 1 if j == tmp['labels'][k] else 0 for j in range(0,10) ]\n",
    "        y_test.append(hot_vector)\n",
    "    #concatenate dataframes by rows    \n",
    "    y_test = np.array( y_test )    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gray preprocess for image\n",
    "def grayscale_cifar(X_train, X_test):\n",
    "    #gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    X_train_g = []\n",
    "    X_test_g = []\n",
    "    for image in X_train:\n",
    "        X_train_g.append(np.dot(image.reshape(32, 32, 3), [0.299, 0.587, 0.114]).reshape(32*32,).astype(float)/255)\n",
    "    for image in X_test:\n",
    "        X_test_g.append(np.dot(image.reshape(32, 32, 3), [0.299, 0.587, 0.114]).reshape(32*32,).astype(float)/255)\n",
    "    return np.array(X_train_g), np.array(X_test_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(which_one):\n",
    "    if which_one == 'mnist':\n",
    "        X_train, X_test, y_train, y_test = load_data_mnist()\n",
    "        #normalize\n",
    "        X_train = X_train.astype(float)/255\n",
    "        X_test = X_test.astype(float)/255       \n",
    "        #add bias vector\n",
    "        X_train = np.hstack( (np.ones((X_train.shape[0],1) ), X_train) )\n",
    "        X_test = np.hstack( (np.ones((X_test.shape[0],1) ), X_test) )\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    elif which_one == 'cifar':\n",
    "        X_train, X_test, y_train, y_test = load_data_cifar()\n",
    "        #preprocess cifar\n",
    "        X_train, X_test = grayscale_cifar(X_train, X_test)\n",
    "        #add bias\n",
    "        X_train = np.hstack( (np.ones((X_train.shape[0],1) ), X_train) )\n",
    "        X_test = np.hstack( (np.ones((X_test.shape[0],1) ), X_test) )\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcheck_softmax(Winit, X, t, lamda):\n",
    "    \n",
    "    W = np.random.rand(*Winit.shape)\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    _list = np.random.randint(X.shape[0], size=5)\n",
    "    x_sample = np.array(X[_list, :])\n",
    "    t_sample = np.array(t[_list, :])\n",
    "    \n",
    "    Ew, gradEw = cost_grad_softmax(W, x_sample, t_sample, lamda)\n",
    "    \n",
    "    \n",
    "    numericalGrad = np.zeros(gradEw.shape)\n",
    "    # Compute all numerical gradient estimates and store them in\n",
    "    # the matrix numericalGrad\n",
    "    for k in range(numericalGrad.shape[0]):\n",
    "        for d in range(numericalGrad.shape[1]):\n",
    "            \n",
    "            #add epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W)\n",
    "            w_tmp[k, d] += epsilon\n",
    "            e_plus, _ = cost_grad_softmax(w_tmp, x_sample, t_sample, lamda)\n",
    "            \n",
    "            #subtract epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W)\n",
    "            w_tmp[k, d] -= epsilon\n",
    "            e_minus, _ = cost_grad_softmax( w_tmp, x_sample, t_sample, lamda)\n",
    "            \n",
    "            #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "    \n",
    "    return ( gradEw, numericalGrad )\n",
    "\n",
    "\n",
    "def gradcheck_activation(hidden_layer_weights, output_layer_weights, output_values_batch, X_train_mini, y_train_mini, activation, lamda, eta):\n",
    "\n",
    "    W = np.random.rand(*hidden_layer_weights.shape)\n",
    "    epsilon = 1e-6\n",
    "    gradEw1 = cost_grad_activation(hidden_layer_weights, output_layer_weights, output_values_batch, X_train_mini, y_train_mini, activation, lamda, eta)\n",
    "    #precalculate new weights\n",
    "    hidden_layer_weights = hidden_layer_weights + eta * gradEw1      \n",
    "    _list = np.random.randint(X_train_mini.shape[0], size=5)\n",
    "    x_sample = np.array(X_train_mini[_list, :])\n",
    "    t_sample = np.array(y_train_mini[_list, :])\n",
    "    numericalGrad = np.zeros(gradEw1.shape)\n",
    "    # Compute all numerical gradient estimates and store them in\n",
    "    # the matrix numericalGrad\n",
    "    for k in range(numericalGrad.shape[0]):\n",
    "        if k % 10 == 0:\n",
    "            print(k)\n",
    "        #pass new hidden activated functions to cost_grad_softmax\n",
    "        #to check changes in LOSS\n",
    "        for d in range(numericalGrad.shape[1]):\n",
    "           \n",
    "            #add epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W)\n",
    "            w_tmp[k, d] += epsilon  \n",
    "            #W1 * X\n",
    "            hidden_values_batch = x_sample.dot((w_tmp+eta*gradEw1).T)\n",
    "            hidden_values_batch_activated = activation_func(activation, hidden_values_batch)\n",
    "            #add bias for output\n",
    "            hidden_values_batch_activated = np.hstack( (np.ones((hidden_values_batch_activated.shape[0],1) ), hidden_values_batch_activated))\n",
    "            e_plus, _ = cost_grad_softmax(output_layer_weights, hidden_values_batch_activated, t_sample, lamda)\n",
    "           \n",
    "            #subtract epsilon to the w[k,d]\n",
    "            w_tmp = np.copy(W)\n",
    "            w_tmp[k, d] -= epsilon\n",
    "            #W1 * X\n",
    "            hidden_values_batch = x_sample.dot((w_tmp+eta*gradEw1).T)\n",
    "            hidden_values_batch_activated = activation_func(activation, hidden_values_batch)\n",
    "            #add bias for output\n",
    "            hidden_values_batch_activated = np.hstack( (np.ones((hidden_values_batch_activated.shape[0],1) ), hidden_values_batch_activated))\n",
    "            e_minus, _ = cost_grad_softmax(output_layer_weights, hidden_values_batch_activated, t_sample, lamda)\n",
    "            \n",
    "            #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "            numericalGrad[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "    \n",
    "            #numericalGrad[k, d] = (sum(sum(e_plus)) - sum(sum(e_minus))) / (2 * epsilon)\n",
    "    \n",
    "    return ( gradEw1, numericalGrad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax activation\n",
    "def softmax( x, ax=1 ):\n",
    "    m = np.max( x, axis=ax, keepdims=True )#max per row\n",
    "    p = np.exp( x - m )\n",
    "    return ( p / np.sum(p,axis=ax,keepdims=True) )\n",
    "\n",
    "#softmax derivative\n",
    "def softmax_der(x, ax):\n",
    "    soft = softmax(x, ax)\n",
    "    return soft*(1 - soft)\n",
    "\n",
    "#activation functions\n",
    "def activation_func(func, a):\n",
    "    if func == 0:\n",
    "        return np.log(1+np.exp(a))\n",
    "    elif func == 1:\n",
    "        return (np.exp(a) - np.exp(-a))/(np.exp(a) + np.exp(-a))\n",
    "    elif func == 2:   \n",
    "        return np.cos(a)\n",
    "    else:\n",
    "        exit(\"Undefined function\")\n",
    "\n",
    "#derivatives of activation functions\n",
    "def activation_func_der(func, a):\n",
    "    if func == 0:\n",
    "        return np.exp(a) / (1 + np.exp(a))\n",
    "    elif func == 1:\n",
    "        return 1 - (activation_func(1, a))**2\n",
    "    elif func == 2:   \n",
    "        return -np.sin(a)\n",
    "    else:\n",
    "        exit(\"Undefined function\")\n",
    "        \n",
    "\n",
    "def cost_grad_softmax(output_layer_weights, hidden_values_batch_activated, y_true, lamda):\n",
    "    output_values_batch = hidden_values_batch_activated.dot(output_layer_weights.T)    \n",
    "    output_values_batch_activated = softmax(output_values_batch)\n",
    "    max_error = np.max(output_values_batch, axis=1)\n",
    "    # Compute the cost function to check convergence\n",
    "    # Using the logsumexp trick for numerical stability - lec8.pdf slide 43\n",
    "    Ew2 = np.sum(y_true * output_values_batch) - np.sum(max_error) - \\\n",
    "        np.sum(np.log(np.sum(np.exp( \\\n",
    "                output_values_batch - np.array([max_error, ] * output_values_batch.shape[1]).T), 1))) - \\\n",
    "        (0.5 * lamda) * np.sum(np.square(output_layer_weights))\n",
    "\n",
    "    # calculate gradient\n",
    "    gradEw2 = (y_true - output_values_batch_activated).T.dot(hidden_values_batch_activated) - lamda * output_layer_weights\n",
    "    return Ew2, gradEw2\n",
    "\n",
    "def cost_grad_activation(hidden_layer_weights, output_layer_weights, output_values_batch, X_train_mini, y_train_mini, activation, lamda, eta):\n",
    "    #activation output\n",
    "    output_values_batch_activated = softmax(output_values_batch)\n",
    "    #derivative activation hidden\n",
    "    derivative_activ_hidden = activation_func_der(activation, hidden_layer_weights.dot(X_train_mini.T)) \n",
    "    #derivative Cost partial derivative\n",
    "    der_cost_part = (y_train_mini - output_values_batch_activated).dot(output_layer_weights) \n",
    "    #W1 gradient\n",
    "    gradEw1 = (derivative_activ_hidden*der_cost_part.T[:-1]).dot(X_train_mini)\n",
    "    return gradEw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights. Divide by 1000 for stability\n",
    "def init_weights(hidden_nodes, dimensions, classes):\n",
    "    hidden_layer_weights = np.random.rand(hidden_nodes,dimensions)/1000\n",
    "    output_layer_weights = np.random.rand(classes, hidden_nodes+1)/1000\n",
    "    return hidden_layer_weights, output_layer_weights\n",
    "\n",
    "\n",
    "def predict(W1, W2, X_test, activation):\n",
    "    # W1 * X\n",
    "    hidden = X_test.dot(W1.T)\n",
    "    hidden = activation_func(activation, hidden)\n",
    "    #Add bias for output\n",
    "    hidden = np.hstack( (np.ones((hidden.shape[0],1) ), hidden) )\n",
    "    # W2 * Output_of_hidden\n",
    "    ytest = softmax( hidden.dot(W2.T) )\n",
    "    ttest = np.argmax(ytest, 1)\n",
    "    print(np.mean( ttest == np.argmax(y_test,1) ))\n",
    "\n",
    "def train(X_train, y_train, options):\n",
    "    #get the options and init weights\n",
    "    lamda, hidden_nodes, activation, eta, batch_size, epochs, verbose, grad_check = options.values()\n",
    "    hidden_layer_weights, output_layer_weights = init_weights(hidden_nodes, X_train.shape[1], y_train.shape[1])\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #loop by batch\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            X_train_mini = X_train[i:i + batch_size]\n",
    "            y_train_mini = y_train[i:i + batch_size]\n",
    "            \n",
    "            #W1 * X\n",
    "            hidden_values_batch = X_train_mini.dot(hidden_layer_weights.T)\n",
    "            hidden_values_batch_activated = activation_func(activation, hidden_values_batch)\n",
    "            #add bias for output\n",
    "            hidden_values_batch_activated = np.hstack( (np.ones((hidden_values_batch_activated.shape[0],1) ), hidden_values_batch_activated) )\n",
    "\n",
    "            #W2 * output_of_hidden\n",
    "            output_values_batch = hidden_values_batch_activated.dot(output_layer_weights.T)    \n",
    "            output_values_batch_activated = softmax(output_values_batch)\n",
    "        \n",
    "            #calculate gradients\n",
    "            Ew2, gradEw2 = cost_grad_softmax(output_layer_weights, hidden_values_batch_activated, y_train_mini, lamda)           \n",
    "            gradEw1 = cost_grad_activation(hidden_layer_weights, output_layer_weights, output_values_batch, X_train_mini, y_train_mini, activation, lamda, eta)\n",
    "            \n",
    "            if verbose:\n",
    "                if i % 30000 == 0:\n",
    "                    print('Epoch:' + str(epoch+1), 'Iteration : %d, Cost function :%f' % (i, Ew2))\n",
    "                #slow\n",
    "                if grad_check:\n",
    "                    print( \"gradEw2 shape: \", gradEw2.shape )                 \n",
    "                    x,y =  gradcheck_softmax(output_layer_weights, hidden_values_batch_activated, y_train_mini, lamda)\n",
    "                    print( \"The difference estimate for gradient of W2 is : \", np.max(np.abs(x-y)))\n",
    "                    #slow..SLOW!!!!!!!!\n",
    "                    print( \"gradEw1 shape: \", gradEw1.shape )  \n",
    "                    x,y =  gradcheck_activation(hidden_layer_weights, output_layer_weights, output_values_batch, X_train_mini, y_train_mini, activation, lamda, eta)\n",
    "                    print( \"The difference estimate for gradient of W1 is : \", np.max(np.abs(x-y)))\n",
    "            \n",
    "            #updates weights\n",
    "            output_layer_weights = output_layer_weights + eta * gradEw2\n",
    "            hidden_layer_weights = hidden_layer_weights + eta * gradEw1\n",
    "            \n",
    "             \n",
    "        if verbose == 0:\n",
    "            print('Epoch:' + str(epoch))\n",
    "    return hidden_layer_weights, output_layer_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Epoch:1\n",
      "Epoch:2\n",
      "Epoch:3\n",
      "Epoch:4\n",
      "Epoch:5\n",
      "Epoch:6\n",
      "Epoch:7\n",
      "Epoch:8\n",
      "Epoch:9\n",
      "Epoch:10\n",
      "Epoch:11\n",
      "Epoch:12\n",
      "Epoch:13\n",
      "Epoch:14\n",
      "Epoch:15\n",
      "Epoch:16\n",
      "Epoch:17\n",
      "Epoch:18\n",
      "Epoch:19\n",
      "Epoch:20\n",
      "Epoch:21\n",
      "Epoch:22\n",
      "Epoch:23\n",
      "Epoch:24\n",
      "Epoch:25\n",
      "Epoch:26\n",
      "Epoch:27\n",
      "Epoch:28\n",
      "Epoch:29\n",
      "Epoch:30\n",
      "Epoch:31\n",
      "Epoch:32\n",
      "Epoch:33\n",
      "Epoch:34\n",
      "Epoch:35\n",
      "Epoch:36\n",
      "Epoch:37\n",
      "Epoch:38\n",
      "Epoch:39\n",
      "Epoch:40\n",
      "Epoch:41\n",
      "Epoch:42\n",
      "Epoch:43\n",
      "Epoch:44\n",
      "Epoch:45\n",
      "Epoch:46\n",
      "Epoch:47\n",
      "Epoch:48\n",
      "Epoch:49\n",
      "Epoch:50\n",
      "Epoch:51\n",
      "Epoch:52\n",
      "Epoch:53\n",
      "Epoch:54\n",
      "Epoch:55\n",
      "Epoch:56\n",
      "Epoch:57\n",
      "Epoch:58\n",
      "Epoch:59\n",
      "0.4051\n"
     ]
    }
   ],
   "source": [
    "#uncomment to load dataset\n",
    "#X_train, X_test, y_train, y_test = load_data('cifar')\n",
    "X_train, X_test, y_train, y_test = load_data('mnist')    \n",
    "X_train, y_train = sklearn.utils.shuffle(X_train, y_train) \n",
    "#set parameters \n",
    "options = {'lamda': .1,\n",
    "           'hidden_nodes': 200,           \n",
    "           'activation':0,\n",
    "           'eta': 1e-4,\n",
    "           'batch_size': 200, \n",
    "           'epochs': 5, \n",
    "           'verbose': 0,\n",
    "           'grad_check': 0}\n",
    "W1, W2 = train(X_train, y_train, options)\n",
    "predict(W1, W2, X_test, options['activation'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" TEST BENCH CROSS_VALIDATION\\nlamda = [.001, .002, .005, .01, .02, .05, .1, .2, .5,]# 1, 2, 5]\\nepochs = [1, 2, 5, 10, 20]\\neta = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]#, 1e-1, 1]\\nbatch_size = [100, 200, 300]\\nhidden_nodes = [ 100, 200]\\nactivations = [1, 2]\\ndata =  ['cifar'] #['mnist',\\n\\nfor i in range(len(data)):\\n    X_train, X_test, y_train, y_test = load_data(data[i])    \\n    X_train, y_train = sklearn.utils.shuffle(X_train, y_train)\\n    for activation in activations:\\n        for h_nodes in hidden_nodes:\\n            for lamd in lamda:\\n                for et in eta:\\n                    for epoch in epochs:\\n                        options = {'lamda': lamd, 'hidden_nodes': h_nodes, 'activation':activation, 'eta': et, 'batch_size': h_nodes, \\n                                   'epochs': epoch, 'verbose': 2, 'grad_check': 0}\\n                        print(options)\\n                        W1, W2 = train(X_train, y_train, options)\\n                        predict(W1, W2, X_test, options['activation'])\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' TEST BENCH CROSS_VALIDATION\n",
    "lamda = [.001, .002, .005, .01, .02, .05, .1, .2, .5,]# 1, 2, 5]\n",
    "epochs = [1, 2, 5, 10, 20]\n",
    "eta = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]#, 1e-1, 1]\n",
    "batch_size = [100, 200, 300]\n",
    "hidden_nodes = [ 100, 200]\n",
    "activations = [1, 2]\n",
    "data =  ['cifar'] #['mnist',\n",
    "\n",
    "for i in range(len(data)):\n",
    "    X_train, X_test, y_train, y_test = load_data(data[i])    \n",
    "    X_train, y_train = sklearn.utils.shuffle(X_train, y_train)\n",
    "    for activation in activations:\n",
    "        for h_nodes in hidden_nodes:\n",
    "            for lamd in lamda:\n",
    "                for et in eta:\n",
    "                    for epoch in epochs:\n",
    "                        options = {'lamda': lamd, 'hidden_nodes': h_nodes, 'activation':activation, 'eta': et, 'batch_size': h_nodes, \n",
    "                                   'epochs': epoch, 'verbose': 2, 'grad_check': 0}\n",
    "                        print(options)\n",
    "                        W1, W2 = train(X_train, y_train, options)\n",
    "                        predict(W1, W2, X_test, options['activation'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
